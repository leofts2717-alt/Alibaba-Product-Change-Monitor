import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import re
import os
import time
from datetime import datetime

# =========================================================================
# ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ é…ç½®åŒº ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡ğŸ‘‡
# =========================================================================

# 1. æ‰«æé¢‘ç‡: 3 åˆ†é’Ÿ
CHECK_INTERVAL = 180 

# 2. ç†”æ–­é˜ˆå€¼ (é˜²æ­¢è¢«åŠ¨ä½ç§»åˆ·å±)
MASSIVE_THRESHOLD = 8

# 3. æ–‡ä»¶è·¯å¾„
CSV_FILE_PATH = r"C:\Users\wlh03\Desktop\AliMonitor\result.csv"

# 4. é”šç‚¹
ANCHOR_INDICES = [10, 20, 30, 40, 48] 

PAGE_KEYWORD = "manage_products"

# =========================================================================

# === æ—¥æœŸæ ‡å‡†åŒ– (å…¼å®¹ Excel) ===
def normalize_date_str(date_str):
    s = str(date_str).strip()
    if not s or s.lower() == 'nan': return ""
    s = s.replace('/', '-')
    try:
        if '-' in s and len(s) <= 10:
            parts = s.split('-')
            if len(parts) == 3:
                return f"{parts[0]}-{int(parts[1]):02d}-{int(parts[2]):02d}"
    except: pass
    return s

async def run():
    print(f">>> æ­£åœ¨å¯åŠ¨ V18.1 (æ™ºèƒ½é”å®š + æ™ºèƒ½å»é‡ç‰ˆ)...")
    print(f">>> ğŸ¯ [æ™ºèƒ½é”å®š] è‡ªåŠ¨å¯»æ‰¾å¹¶æ¿€æ´»é˜¿é‡Œåå°çª—å£")
    print(f">>> â™»ï¸ [é€»è¾‘å‡çº§] æ–°ID + æ—§å‹å· = è€å“é‡å‘")
    print(f">>> ğŸ”¥ [é€»è¾‘å‡çº§] æ–°ID + æ–°å‹å· = æ–°å“å‘å¸ƒ")
    print(f">>> ğŸ› ï¸ [è‡ªåŠ¨ä¿®å¤] å·²å¼€å¯æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–")

    # åˆå§‹åŒ–åº“
    content_snapshot = {} 
    history_time_db = {} 
    rank_snapshot = {}
    seen_models = set()
    
    last_anchors = []

    # === åŠ è½½å†å²æ¡£æ¡ˆ ===
    if os.path.exists(CSV_FILE_PATH):
        try:
            df_hist = pd.read_csv(CSV_FILE_PATH, dtype=str)
            for _, row in df_hist.iterrows():
                p_id = str(row.get('ID', '')).strip()
                if p_id:
                    # 1. åŸºç¡€æ•°æ®
                    raw_title = str(row.get('æ ‡é¢˜', '')).strip()
                    raw_price = str(row.get('ä»·æ ¼', '')).strip()
                    raw_model = str(row.get('å‹å·', '')).strip()
                    raw_owner = str(row.get('è´Ÿè´£äºº', '')).strip()
                    raw_time = str(row.get('Aliæ›´æ–°æ—¶é—´', '')).strip()
                    
                    # 2. æ ‡å‡†åŒ–æ—¶é—´
                    norm_time = normalize_date_str(raw_time)
                    
                    # 3. å­˜å…¥æŒ‡çº¹
                    fingerprint = f"{raw_title}_{raw_price}_{raw_model}_{raw_owner}_{norm_time}"
                    content_snapshot[p_id] = fingerprint
                    
                    # 4. å­˜å…¥æ—¶é—´åº“
                    if norm_time: history_time_db[p_id] = norm_time
                    
                    # 5. å­˜å…¥å‹å·åº“ (å¿½ç•¥å¤§å°å†™)
                    if raw_model: seen_models.add(raw_model.upper())
                        
            print(f">>> ğŸ“š å†å²åº“è½½å…¥å®Œæ¯•ï¼šç›‘æ§ {len(content_snapshot)} ä¸ªIDï¼Œ{len(seen_models)} ä¸ªç‹¬ç«‹å‹å·")
        except Exception as e:
            print(f">>> âš ï¸ å†å²åº“è¯»å–è­¦å‘Š: {e}")

    async with async_playwright() as p:
        try:
            browser = await p.chromium.connect_over_cdp("http://localhost:9222")
            print(">>> âœ… æˆåŠŸè¿æ¥åˆ°æµè§ˆå™¨ï¼")
        except Exception as e:
            print(f">>> âŒ è¿æ¥å¤±è´¥: {e}")
            return

        context = browser.contexts[0]
        if not context.pages:
            print(">>> âŒ æµè§ˆå™¨æ²¡æœ‰æ‰“å¼€ä»»ä½•é¡µé¢ï¼")
            return

        # ==========================================
        # ğŸ”¥ V18.1 æ ¸å¿ƒå‡çº§ï¼šè‡ªåŠ¨å¯»æ‰¾ç›®æ ‡çª—å£
        # ==========================================
        target_page = None
        print(f">>> æ­£åœ¨ {len(context.pages)} ä¸ªæ ‡ç­¾é¡µä¸­å¯»æ‰¾é˜¿é‡Œåå°...")
        
        for p_tab in context.pages:
            try:
                # ç®€å•åˆ¤æ–­ URL æ˜¯å¦åŒ…å«å…³é”®å­—
                if PAGE_KEYWORD in p_tab.url:
                    target_page = p_tab
                    # è·å–æ ‡é¢˜åªä¸ºäº†æ‰“å°å¥½çœ‹ï¼Œå‡ºé”™ä¹Ÿä¸å½±å“é€»è¾‘
                    try: 
                        title = await p_tab.title()
                        print(f"    - å‘½ä¸­: [{title}]")
                    except: 
                        print(f"    - å‘½ä¸­: [æœªçŸ¥æ ‡é¢˜]")
                    break
            except: pass
        
        if not target_page:
            print(f">>> âŒ æœªæ‰¾åˆ°åŒ…å« '{PAGE_KEYWORD}' çš„é¡µé¢ã€‚è¯·ç¡®ä¿ä½ å·²ç»æ‰“å¼€äº†é˜¿é‡Œå•†å“ç®¡ç†åå°ï¼")
            return
        
        # æ¿€æ´»è¯¥é¡µé¢ï¼Œè®¾ä¸ºå½“å‰æ“ä½œå¯¹è±¡
        page = target_page
        await page.bring_to_front()
        print(">>> ğŸ¯ çª—å£é”å®šæˆåŠŸï¼å¼€å§‹ç›‘æ§...")
        # ==========================================

        SCROLL_CONTAINER = ".pp-layout-content"

        while True:
            try:
                # Double Check: ç¡®ä¿é¡µé¢æ²¡è·‘å
                if PAGE_KEYWORD in page.url:
                    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸš€ å¼€å§‹æ–°ä¸€è½®æ‰«æ...")

                    # 1. å½’ä½
                    try:
                        btn_page_1 = page.locator('button[aria-label*="ç¬¬1é¡µ"]')
                        if await btn_page_1.count() == 0:
                            btn_page_1 = page.locator('.next-pagination-list button').filter(has_text=re.compile(r"^1$"))
                        if await btn_page_1.count() > 0:
                            class_attr = await btn_page_1.get_attribute("class")
                            if class_attr and "next-current" not in class_attr:
                                await btn_page_1.click()
                                await page.wait_for_timeout(4000)
                    except: pass

                    # 2. åˆ·æ–°
                    try:
                        await page.reload(timeout=90000, wait_until='domcontentloaded')
                        await page.wait_for_selector('.list-item', timeout=30000)
                    except:
                        print(f">>> âš ï¸ åˆ·æ–°è¶…æ—¶ï¼Œè·³è¿‡...")
                        await asyncio.sleep(5)
                        continue 

                    # 3. 50æ¡
                    try:
                        btn_50 = page.locator(".next-pagination-size-selector-btn").filter(has_text="50")
                        if await btn_50.count() > 0:
                            await btn_50.click()
                            await page.wait_for_timeout(3000)
                    except: pass

                    # 4. æ»šåŠ¨
                    try:
                        await page.evaluate(f"document.querySelector('{SCROLL_CONTAINER}').scrollTop = 0")
                        await page.wait_for_timeout(1000)
                        scroll_info = await page.evaluate(f"() => {{ return {{ scrollHeight: document.querySelector('{SCROLL_CONTAINER}').scrollHeight }}; }}")
                        total_height = scroll_info['scrollHeight']
                        current_pos = 0
                        while current_pos < total_height:
                            current_pos += 600
                            await page.evaluate(f"document.querySelector('{SCROLL_CONTAINER}').scrollTop = {current_pos}")
                            await page.wait_for_timeout(300) 
                            if len(await page.locator('.list-item').all()) >= 50: break
                        await page.evaluate(f"document.querySelector('{SCROLL_CONTAINER}').scrollTop = 0")
                    except: pass

                    rows = await page.locator('.list-item').all()
                    if not rows: continue
                    
                    current_page_ids_set = set()
                    row_data_list = [] 

                    for row in rows:
                        text_content = await row.inner_text()
                        id_match = re.search(r'ID:\s*(\d+)', text_content)
                        if id_match:
                            p_id = id_match.group(1)
                            current_page_ids_set.add(p_id)
                            row_data_list.append((row, p_id))

                    # å¹½çµè¡¥å¿å‡†å¤‡
                    missing_ids_map = {} 
                    if rank_snapshot:
                        for old_id, old_rank in rank_snapshot.items():
                            if old_id not in current_page_ids_set:
                                missing_ids_map[old_rank] = old_id
                    
                    # =================================================
                    # ğŸ”¥ é˜¶æ®µäºŒ: ç­›é€‰ä¸æ¯”å¯¹
                    # =================================================
                    
                    candidates = [] 
                    found_boundary = False 
                    current_run_all_ids = [] 
                    current_run_rank_map = {}
                    global_rank_counter = 0

                    for row, p_id in row_data_list:
                        current_run_all_ids.append(p_id)
                        current_run_rank_map[p_id] = global_rank_counter
                        current_rank = global_rank_counter
                        global_rank_counter += 1

                        # --- æå– ---
                        title = "æœªæ‰¾åˆ°æ ‡é¢˜"
                        link = ""
                        try:
                            subject_div = row.locator('.product-subject')
                            if await subject_div.count() > 0:
                                a_tag = subject_div.locator('a').first
                                if await a_tag.count() > 0:
                                    link = await a_tag.get_attribute('href') or ""
                                    if link and not link.startswith('http'): link = "https:" + link
                                    pre_tag = a_tag.locator('pre')
                                    if await pre_tag.count() > 0: title = await pre_tag.inner_text()
                                    else: title = await a_tag.inner_text()
                        except: pass
                        title = title.strip()

                        model = ""
                        try:
                            model_el = row.locator('.product-model')
                            if await model_el.count() > 0:
                                raw = await model_el.inner_text()
                                model = raw.replace("å‹å·:", "").replace("Model:", "").strip()
                        except: pass

                        price_val, owner_val, ali_time_val = "", "", ""
                        try:
                            cols = await row.locator('.next-col').all()
                            if len(cols) >= 6:
                                price_val = await cols[3].inner_text()
                                owner_val = await cols[4].inner_text()
                                ali_time_val = await cols[5].inner_text()
                        except: pass
                        
                        price_val = price_val.strip()
                        owner_val = owner_val.strip()
                        ali_time_val = ali_time_val.strip()
                        norm_ali_time = normalize_date_str(ali_time_val)

                        current_fingerprint = f"{title}_{price_val}_{model}_{owner_val}_{norm_ali_time}"
                        
                        is_recorded = False
                        status = ""
                        emoji = ""

                        # ==========================================
                        # ğŸ”¥ é€»è¾‘: åŒºåˆ†æ–°å“ä¸é‡å‘
                        # ==========================================
                        if p_id not in content_snapshot:
                            is_recorded = True
                            if model.upper() in seen_models:
                                status = "è€å“é‡å‘" 
                                emoji = "â™»ï¸"
                            else:
                                status = "æ–°å“å‘å¸ƒ"
                                emoji = "ğŸ”¥"
                        else:
                            old_fingerprint = content_snapshot[p_id]
                            if current_fingerprint != old_fingerprint:
                                is_recorded = True
                                status = "ä¿®æ”¹è¯¦æƒ…"
                                emoji = "âœï¸ (å†…å®¹å˜åŠ¨)"
                            else:
                                # æ’åé€»è¾‘
                                if p_id in rank_snapshot:
                                    old_rank = rank_snapshot[p_id]
                                    ghosts_above = 0
                                    for missing_rank in missing_ids_map.keys():
                                        if missing_rank < old_rank:
                                            ghosts_above += 1
                                    expected_rank = old_rank - ghosts_above
                                    
                                    if current_rank < expected_rank:
                                        is_recorded = True
                                        status = "ä¿®æ”¹è¯¦æƒ…"
                                        emoji = "ğŸš€ (æ’åä¸Šå‡)"

                        if is_recorded:
                            candidates.append({
                                'ID': p_id,
                                'å‹å·': model,
                                'å˜åŒ–æƒ…å†µ': status,
                                'Aliæ›´æ–°æ—¶é—´': ali_time_val, 
                                'norm_time': norm_ali_time,
                                'å•†å“é“¾æ¥': link,
                                'æ ‡é¢˜': title,
                                'ä»·æ ¼': price_val,
                                'æŠ“å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'è´Ÿè´£äºº': owner_val,
                                'log_str': f"{emoji} [{status}] {model} | {ali_time_val}",
                                'fingerprint': current_fingerprint
                            })
                            if model: seen_models.add(model.upper())

                        if last_anchors and p_id in last_anchors:
                            found_boundary = True
                            break
                    
                    found_boundary = True 

                    # =================================================
                    # ğŸ”¥ é˜¶æ®µä¸‰: æ™ºèƒ½ç†”æ–­
                    # =================================================
                    final_items = []
                    
                    if len(candidates) >= MASSIVE_THRESHOLD:
                        print(f"    >>> ğŸ›¡ï¸ è§¦å‘ç†”æ–­æ£€æŸ¥: å˜åŠ¨æ•° {len(candidates)}")
                        count_filtered = 0
                        for item in candidates:
                            # æ–°å“ã€é‡å‘ã€å†…å®¹å˜åŠ¨ -> æ”¾è¡Œ
                            if "æ–°å“" in item['å˜åŒ–æƒ…å†µ'] or "é‡å‘" in item['å˜åŒ–æƒ…å†µ']:
                                final_items.append(item)
                                continue
                            
                            # æŒ‡çº¹å˜åŠ¨ -> æ”¾è¡Œ
                            p_id = item['ID']
                            old_fp = content_snapshot.get(p_id)
                            if old_fp and item['fingerprint'] != old_fp:
                                final_items.append(item)
                                continue
                                
                            # æ’åä¸Šå‡ -> æŸ¥æ—¶é—´
                            hist_time = history_time_db.get(p_id)
                            current_norm_time = item['norm_time']
                            if hist_time and hist_time == current_norm_time:
                                count_filtered += 1 
                            else:
                                final_items.append(item)
                        print(f"    >>> ğŸ§¹ è¿‡æ»¤ {count_filtered} æ¡è¢«åŠ¨ä½ç§»ï¼Œä¿ç•™ {len(final_items)} æ¡ã€‚")
                    else:
                        final_items = candidates

                    # === æ”¶å°¾ ===
                    new_anchors = []
                    if current_run_all_ids:
                        for idx in ANCHOR_INDICES:
                            if idx < len(current_run_all_ids):
                                new_anchors.append(current_run_all_ids[idx])
                    if new_anchors: last_anchors = new_anchors

                    if current_run_rank_map:
                        rank_snapshot.update(current_run_rank_map)

                    if final_items:
                        for item in final_items:
                            print(item['log_str'])
                            content_snapshot[item['ID']] = item['fingerprint']
                            history_time_db[item['ID']] = item['norm_time']
                            if 'norm_time' in item: del item['norm_time']
                            if 'log_str' in item: del item['log_str']
                            if 'fingerprint' in item: del item['fingerprint']

                        df = pd.DataFrame(final_items)
                        column_order = ['ID', 'å‹å·', 'å˜åŒ–æƒ…å†µ', 'Aliæ›´æ–°æ—¶é—´', 'å•†å“é“¾æ¥', 'æ ‡é¢˜', 'ä»·æ ¼', 'æŠ“å–æ—¶é—´', 'è´Ÿè´£äºº']
                        df = df[column_order]
                        header = not os.path.exists(CSV_FILE_PATH)
                        df.to_csv(CSV_FILE_PATH, mode='a', header=header, index=False, encoding='utf-8-sig')
                        print(f"    >>> ğŸ‰ æˆåŠŸè®°å½• {len(final_items)} æ¡æ•°æ®ï¼")
                    else:
                        if not candidates: print("    >>> ğŸƒ æ— å˜åŒ–ã€‚")

                print(f">>> ğŸ’¤ å¾…æœºä¸­...")
                remaining_time = CHECK_INTERVAL
                step = 60
                while remaining_time > 0:
                    await asyncio.sleep(min(remaining_time, step))
                    remaining_time -= step
                    if remaining_time > 0: print(f">>> â³ å€’è®¡æ—¶: {remaining_time} ç§’...")

            except Exception as e:
                print(f"!!! é”™è¯¯: {e}")
                await asyncio.sleep(60)

if __name__ == '__main__':
    asyncio.run(run())